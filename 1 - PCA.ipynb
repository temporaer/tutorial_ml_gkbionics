{
 "metadata": {
  "name": "1 - PCA"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generating Data\n",
      "===============\n",
      "\n",
      "We first create 200 random two-dimensional data points.\n",
      "The data points are sampled from a multinomial normal distribution.\n",
      "\n",
      "You don't have to understand precisely how we do this. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Cov = np.array([[2.9, -2.2], [-2.2, 6.5]])\n",
      "X = np.random.multivariate_normal([1,2], Cov, size=200)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's have a look at the raw data first..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.set_printoptions(4, suppress=True) # show only four decimals\n",
      "print X[:10,:] # print the first 10 rows of D (from 0 to 9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do you see a relationship between the two columns? Tricky, I'd say. However, since the data is two-dimensional, we can plot the data, which allows us to see the relationship."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(4,4))\n",
      "plt.scatter(X[:,0], X[:,1])\n",
      "plt.axis('equal') # equal scaling on both axis;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also have a look at the actual covariance matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.cov(X,rowvar=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Running PCA\n",
      "===========\n",
      "\n",
      "We would now like to analyze the directions in which the data varies most. For that, we \n",
      "\n",
      "1. place the point cloud in the center (0,0) and\n",
      "2. rotate it, such that the direction with most variance is parallel to the x-axis.\n",
      "\n",
      "Both steps can be done using PCA, which is conveniently available in sklearn.\n",
      "\n",
      "We start by loading the PCA class from the sklearn package and creating an instance of the class:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, `pca` is an object which has a function `pca.fit_transform(x)` which performs both steps from above to its argument `x`, and returns the centered and rotated version of `x`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_pca = pca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.components_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.mean_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(4,4))\n",
      "plt.scatter(X_pca[:,0], X_pca[:,1])\n",
      "plt.axis('equal');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The covariances between different axes should be zero now. We can double-check by having a look at the non-diagonal entries of the covariance matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.cov(X_pca, rowvar=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "High-Dimensional Data\n",
      "=====================\n",
      "\n",
      "Our small example above was very easy, since we could get insight into the data by simply plotting it. This approach will not work once you have more than 3 dimensions, let's say we have the same data, but it is represented in four dimensions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(1)\n",
      "X_HD = np.dot(X,np.random.uniform(0.2,3,(2,4))*(np.random.randint(0,2,(2,4))*2-1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets look at the data again. First, the raw data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X_HD[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That one is more tricky. See anything? We can also try plot a few two-dimensional projections:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(8,8))\n",
      "for i in xrange(4):\n",
      "    for j in xrange(4):\n",
      "        plt.subplot(4, 4, i * 4 + j + 1)\n",
      "        plt.scatter(X_HD[:,i], X_HD[:,j])\n",
      "        plt.axis('equal')\n",
      "        plt.gca().set_aspect('equal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is not easy to see that this is still a two-dimensional dataset! \n",
      "\n",
      "However, if we now do PCA on it, you'll see that the last two dimensions do not matter at all:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_HE = pca.fit_transform(X_HD)\n",
      "print X_HE[:10,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here it is easy to see, that the data is **still only two-dimensional**. Let's plot the two dimensions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(4,4))\n",
      "plt.scatter(X_HE[:,0], X_HE[:,1])\n",
      "plt.axis('equal')\n",
      "plt.gca().set_aspect('equal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why does the result look differently than the two-dimensional data from which we generated it?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(8,8))\n",
      "for i in xrange(4):\n",
      "    for j in xrange(4):\n",
      "        plt.subplot(4, 4, i * 4 + j + 1)\n",
      "        plt.scatter(X_HE[:,i], X_HE[:,j])\n",
      "        plt.gca().set_xlim(-40,40)\n",
      "        plt.gca().set_ylim(-40,40)\n",
      "        plt.axis('equal')\n",
      "        plt.gca().set_aspect('equal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dimension Reduction with PCA\n",
      "============================\n",
      "\n",
      "We can see that there are actually only two dimensions in the dataset. \n",
      "\n",
      "Let's throw away even more data -- the second dimension -- and reconstruct the original data in `D`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = PCA(1) # only keep one dimension!\n",
      "X_E = pca.fit_transform(X_HD)\n",
      "print X_E[:10,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets plot the reconstructed data and compare to the original data D. We plot the original data in red, and the reconstruction with only one dimension in blue:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_reconstructed = pca.inverse_transform(X_E)\n",
      "plt.figure(figsize=(8,8))\n",
      "for i in xrange(4):\n",
      "    for j in xrange(4):\n",
      "        plt.subplot(4, 4, i * 4 + j + 1)\n",
      "        plt.scatter(X_HD[:,i], X_HD[:,j],c='r')\n",
      "        plt.scatter(X_reconstructed[:,i], X_reconstructed[:,j],c='b')\n",
      "        plt.axis('equal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "PCA on Images"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this final example, we use the $k$-Means algorithm on the classical MNIST dataset.\n",
      "\n",
      "The MNIST dataset contains images of hand-written digits. \n",
      "\n",
      "Let's first fetch the dataset from the internet (which may take a while, note the asterisk [*]):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_mldata\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.utils import shuffle\n",
      "X_digits, _,_, Y_digits = fetch_mldata(\"MNIST Original\").values() # fetch dataset from internet\n",
      "X_digits, Y_digits = shuffle(X_digits,Y_digits) # shuffle dataset (which is ordered!)\n",
      "X_digits = X_digits[-5000:]       # take only the last instances, to shorten runtime of PCA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's have a look at some of the instances in the dataset we just loaded:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.rc(\"image\", cmap=\"binary\")\n",
      "plt.figure(figsize=(8,4))\n",
      "for i in xrange(10):\n",
      "    plt.subplot(2,5,i+1)\n",
      "    plt.imshow(X_digits[i].reshape(28,28))\n",
      "    plt.xticks(())\n",
      "    plt.yticks(())\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Warning**: This takes quite a few seconds, so be patient until the asterisk [*] disappears!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA()\n",
      "X2_digits = pca.fit_transform(X_digits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It does not make much sense to look at the transformed images, they will look like noise to us. \n",
      "\n",
      "Instead, let's have a look at the most important directions on which the dataset was projected:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(8,6))\n",
      "W = pca.components_\n",
      "\n",
      "for i in xrange(10): # loop over all means\n",
      "    plt.subplot(2,5,i+1)\n",
      "    plt.imshow(W[i].reshape(28,28))\n",
      "    plt.xticks(())\n",
      "    plt.yticks(())\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The later directions (here, from the 100-th on) mainly show noise, small variations between different, but very similar training instances:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(8,4))\n",
      "W = pca.components_\n",
      "\n",
      "for i in xrange(10): # loop over all means\n",
      "    plt.subplot(2,5,i+1)\n",
      "    plt.imshow(W[100+i].reshape(28,28))\n",
      "    plt.xticks(())\n",
      "    plt.yticks(())\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The number of \"interesting\" dimensions can be seen from the importance of the found directions. \n",
      "\n",
      "We can simply plot them:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(pca.explained_variance_);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that the intrinsic dimensionality is not higher than maybe 100, even though the dataset has 784 dimensions!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's reconstruct the data again using only a handfull of the 784 dimensions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(20)\n",
      "X2_few_digits = pca.fit_transform(X_digits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(16,4))\n",
      "X_recons_digits = pca.inverse_transform(X2_few_digits)\n",
      "for i in xrange(10):\n",
      "    plt.subplot(2,10,i+1)\n",
      "    plt.imshow(X_recons_digits[i].reshape(28,28))\n",
      "    plt.xticks(())\n",
      "    plt.yticks(())\n",
      "for i in xrange(10):\n",
      "    plt.subplot(2,10,11+i)\n",
      "    plt.imshow(X_digits[i].reshape(28,28))\n",
      "    plt.xticks(())\n",
      "    plt.yticks(())\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Playing around with this Notebook"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- What happens, when you multiply one of the data axis with a large (or small) number? \n",
      "\n",
      "  e.g. using X[:,0] *= 100\n",
      "\n",
      "  Does the result stay the same? Why/why not?\n",
      "\n",
      "-----\n",
      "\n",
      "- Try to explore the iris dataset below using PCA. \n",
      "\n",
      "  What happens if you visualize the *last* components of PCA instead of the first ones?\n",
      "\n",
      "-----\n",
      "\n",
      "- Use PCA with 1, 2 or 3 axes and reconstruct the Iris data from each. How do the results change? Show plots!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "_,data,target,_,_ = datasets.load_iris().values()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}